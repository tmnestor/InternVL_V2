# Configuration for multimodal receipt counter
# This config extends the base receipt counter with vision-language capabilities

# Random seed for reproducibility
seed: 42
log_level: INFO

# Model configuration
model:
  # For InternVL2 model
  # pretrained_path: "$HOME/nfs_share/models/huggingface/hub/InternVL2_5-1B"
  # pretrained_path: "/Users/tod/PretrainedLLM/InternVL2_5-1B"
  
  # Using InternVL2 model - we need a model with vision encoder and language encoder/decoder
  pretrained_path: "/home/jovyan/nfs_share/models/huggingface/hub/InternVL2_5-1B"
  multimodal: true  # Enable multimodal mode
  num_classes: 3  # 0, 1, 2+ receipts for classification task
  use_8bit: false  # Disable 8-bit quantization for better accuracy
  
  # Classification head configuration (for receipt counting)
  classifier:
    hidden_dims: [512, 256]
    dropout_rates: [0.2, 0.1]
    batch_norm: true
    activation: "gelu"

# Data configuration
data:
  train_csv: "data/multimodal/train/metadata.csv"
  train_dir: "data/multimodal/train/images"
  val_csv: "data/multimodal/val/metadata.csv"
  val_dir: "data/multimodal/val/images"
  test_csv: "data/multimodal/test/metadata.csv"
  test_dir: "data/multimodal/test/images"
  batch_size: 16
  num_workers: 1
  image_size: 448  # InternVL2 default size
  augmentation: true
  max_text_length: 128  # Maximum length for text sequences

# Training configuration
training:
  epochs: 20
  learning_rate: 2.0e-5
  weight_decay: 1.0e-4
  warmup_steps: 500
  gradient_accumulation_steps: 1
  flash_attention: false  # Disable Flash Attention since it's not available
  fp16: false  # Disable mixed precision training for better stability
  gradient_clip: 1.0  # Gradient clipping value
  torch_compile: false  # Enable/disable torch.compile optimization
  
  # Multi-stage training settings
  three_stage:
    enabled: true
    
    # Stage 2: Unfreeze vision encoder, train with small LR
    stage2:
      start_epoch: 6
      lr_multiplier: 0.01
    
    # Stage 3: Full fine-tuning with balanced LRs
    stage3:
      start_epoch: 11
      lr_multiplier: 0.01
  
  # Loss weights
  loss_weights:
    classification: 1.0
    language: 1.0
  
  # Optimizer settings
  optimizer:
    name: "adamw"
    learning_rate: 2.0e-5
    weight_decay: 0.01
    language_lr_multiplier: 0.1
    backbone_lr_multiplier: 0.01
  
  # Learning rate scheduler
  scheduler:
    name: "cosine"
    min_lr_factor: 0.1
    warmup_steps: 500
  
  # Early stopping
  early_stopping:
    patience: 5
    min_delta: 0.01

# Output configuration
output:
  model_dir: "models/multimodal"
  results_dir: "results/multimodal"
  tensorboard: true
  save_best_only: false
  save_frequency: 1
  checkpoint_frequency: 1
  log_dir: "logs/multimodal"

# Evaluation configuration
evaluation:
  model_path: "models/multimodal/best_model.pt"
  batch_size: 16
  save_predictions: true
  metrics:
    # Classification metrics
    accuracy: true
    precision: true
    recall: true
    f1: true
    
    # Generation metrics
    bleu: true
    rouge: true
    perplexity: false  # Disable if computation is too expensive