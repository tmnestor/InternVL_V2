# Configuration for unified multimodal receipt counter
# This config supports both receipt counting and question answering with unified dataset

# Random seed for reproducibility
seed: 42
log_level: INFO

# Model configuration
model:
  # For InternVL2 model
  pretrained_path: "/Users/tod/PretrainedLLM/InternVL2_5-1B"
  multimodal: true  # Enable multimodal mode
  num_classes: 3  # 0, 1, 2+ receipts for classification task
  use_8bit: false  # Disable 8-bit quantization for better accuracy
  
  # Classification head configuration (for receipt counting)
  classifier:
    hidden_dims: [512, 256]
    dropout_rates: [0.2, 0.1]
    batch_norm: true
    activation: "gelu"

# Data configuration
data:
  # Using a unified dataset structure
  root_dir: "data/multimodal"  # Directory containing train/val/test splits
  batch_size: 4  # Reduced batch size to address OOM errors
  num_workers: 1
  image_size: 448  # InternVL2 default size
  augmentation: true
  max_text_length: 128  # Maximum length for text sequences

# Training configuration
training:
  epochs: 20
  learning_rate: 1.0e-7  # Reduced due to persistent high losses
  weight_decay: 1.0e-4
  warmup_steps: 1000  # Increased warmup to improve initial stability
  gradient_accumulation_steps: 4  # Increased to maintain effective batch size
  flash_attention: false  # Disable Flash Attention since it's not available
  fp16: false  # Disable mixed precision training for better stability
  gradient_clip: 1.0  # Gradient clipping value
  torch_compile: false  # Enable/disable torch.compile optimization
  memory_efficient: true  # Enable memory efficiency optimizations
  low_cpu_mem_usage: true  # Reduce CPU memory usage
  
  # Multi-stage training settings
  three_stage:
    enabled: true
    
    # Stage 2: Unfreeze vision encoder, train with small LR
    stage2:
      start_epoch: 6
      lr_multiplier: 0.01
    
    # Stage 3: Full fine-tuning with balanced LRs
    stage3:
      start_epoch: 11
      lr_multiplier: 0.01
  
  # Task weighting
  loss_weights:
    classification: 1.0   # Receipt counting task weight
    language: 1.0         # Question answering task weight
  
  # Optimizer settings
  optimizer:
    name: "adamw"
    learning_rate: 2.0e-5
    weight_decay: 0.01
    language_lr_multiplier: 0.1
    backbone_lr_multiplier: 0.01
  
  # Learning rate scheduler
  scheduler:
    name: "cosine"
    min_lr_factor: 0.1
    warmup_steps: 500
  
  # Early stopping
  early_stopping:
    patience: 5
    min_delta: 0.01

# Output configuration
output:
  model_dir: "models/unified_multimodal"
  results_dir: "results/unified_multimodal"
  tensorboard: true
  save_best_only: false
  save_frequency: 1
  checkpoint_frequency: 1
  log_dir: "logs/unified_multimodal"

# Evaluation configuration
evaluation:
  model_path: "models/unified_multimodal/best_model.pt"
  batch_size: 16
  save_predictions: true
  metrics:
    # Classification metrics
    accuracy: true
    precision: true
    recall: true
    f1: true
    
    # Generation metrics
    bleu: true
    rouge: true
    perplexity: false  # Disable if computation is too expensive