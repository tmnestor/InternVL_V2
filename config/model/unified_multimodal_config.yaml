# Configuration for unified multimodal receipt counter
# This config supports both receipt counting and question answering with unified dataset

# Random seed for reproducibility
seed: 42
log_level: INFO

# Model configuration
model:
  # For InternVL2 model
  pretrained_path: "/Users/tod/PretrainedLLM/InternVL2_5-1B"
  # pretrained_path: "$HOME/nfs_share/models/huggingface/hub/InternVL2_5-1B"
  multimodal: true  # Enable multimodal mode
  num_classes: 3  # 0, 1, 2+ receipts for classification task
  use_8bit: false  # Disable 8-bit quantization for better accuracy
  
  # Classification head configuration (for receipt counting)
  classifier:
    hidden_dims: [512, 256]
    dropout_rates: [0.2, 0.1]
    batch_norm: true
    activation: "gelu"

# Data configuration
data:
  # Using a unified dataset structure
  root_dir: "data/multimodal"  # Directory containing train/val/test splits
  batch_size: 4  # Reduced batch size to address OOM errors
  num_workers: 1
  image_size: 448  # InternVL2 default size
  augmentation: true
  max_text_length: 128  # Maximum length for text sequences

# Training configuration
training:
  epochs: 15
  learning_rate: 5.0e-5  # Increased for faster progress
  weight_decay: 1.0e-4
  warmup_steps: 200  # Reduced for faster warmup
  gradient_accumulation_steps: 4  # Maintained to balance memory and updates
  flash_attention: false  # Disable Flash Attention since it's not available
  fp16: false  # Disable mixed precision training for better stability
  gradient_clip: 1.0  # Gradient clipping value
  torch_compile: false  # Enable/disable torch.compile optimization
  memory_efficient: true  # Enable memory efficiency optimizations
  low_cpu_mem_usage: true  # Reduce CPU memory usage
  
  # Multi-stage training settings
  three_stage:
    enabled: true
    
    # Stage 2: Unfreeze vision encoder, train with small LR
    stage2:
      start_epoch: 4  # Earlier transition for faster adaptation
      lr_multiplier: 0.05  # Increased from 0.01 for better learning
    
    # Stage 3: Full fine-tuning with balanced LRs
    stage3:
      start_epoch: 8  # Earlier transition from 11
      lr_multiplier: 0.05  # Increased from 0.01 for better learning
  
  # Task weighting
  loss_weights:
    classification: 1.0   # Receipt counting task weight
    language: 1.0         # Question answering task weight
  
  # Optimizer settings
  optimizer:
    name: "adamw"
    learning_rate: 5.0e-5  # Increased and matched with main learning_rate
    weight_decay: 0.01
    language_lr_multiplier: 0.2  # Increased from 0.1 for faster language model training
    backbone_lr_multiplier: 0.05  # Increased from 0.01 for faster backbone training
  
  # Learning rate scheduler
  scheduler:
    name: "cosine"
    min_lr_factor: 0.01  # Reduced for more aggressive decay schedule
    warmup_steps: 200  # Matched with main warmup_steps
  
  # Early stopping
  early_stopping:
    patience: 3
    min_delta: 0.01

# Output configuration
output:
  model_dir: "models/unified_multimodal"
  results_dir: "results/unified_multimodal"
  tensorboard: true
  save_best_only: false
  save_frequency: 1
  checkpoint_frequency: 1
  log_dir: "logs/unified_multimodal"

# Evaluation configuration
evaluation:
  model_path: "models/unified_multimodal/best_model.pt"
  batch_size: 16
  save_predictions: true
  metrics:
    # Classification metrics
    accuracy: true
    precision: true
    recall: true
    f1: true
    
    # Generation metrics
    bleu: true
    rouge: true
    perplexity: false  # Disable if computation is too expensive